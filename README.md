# ğŸ“ˆ **The Convergence of Machine Learning and HPC in Stock Market Analysis: LSTM RNNs, GPU Acceleration, and Multi-worker Strategies
** ğŸ“‰

## ğŸ¥ **Video Explanation**
[![Watch the Video](https://img.youtube.com/vi/VIDEO_ID/maxresdefault.jpg)](https://www.youtube.com/watch?v=VIDEO_ID)

---

## ğŸ“ **Table of Contents**
1. [ğŸ” Introduction](#-introduction)
2. [ğŸš€ Features](#-features)
3. [ğŸ“¦ Install Required Packages](#-install-required-packages)
4. [ğŸ› ï¸ Usage](#ï¸-usage)
5. [ğŸ“Š Results](#-results)
6. [ğŸ‘¥ Multi-Worker Strategy](#-multi-worker-strategy)
7. [â±ï¸ Profiling & Performance](#ï¸-profiling--performance)
8. [ğŸ”’ Security](#-security)
9. [ğŸ“„ License](#-license)
10. [ğŸ™ Acknowledgements](#-acknowledgements)

---

## ğŸ” **Introduction**

Welcome to our **Stock Price Prediction** HPC final project! ğŸ“ˆâœ¨ This repository contains a Jupyter Notebook (`.ipynb`) that demonstrates how to predict stock prices using a **Long Short-Term Memory (LSTM)** neural network, enhanced with **GPU acceleration** via PyCUDA and a **multi-worker synchronization strategy** leveraging AWS S3. 

---

## ğŸš€ **Features**

- **Data Acquisition & Visualization** ğŸ“Š: Fetches historical stock data using `yfinance` and visualizes trends with moving averages.
- **GPU-Accelerated Preprocessing** ğŸ–¥ï¸âš¡: Utilizes PyCUDA to perform Min-Max Scaling, speeding up data preprocessing.
- **Deep LSTM Architecture** ğŸ§ ğŸ”—: Implements a multi-layer LSTM model with dropout regularization to capture complex temporal dependencies.
- **Distributed Training** ğŸ‘¥ğŸŒ: Employs a multi-worker strategy with AWS S3 for model synchronization and weight averaging.
- **Performance Profiling** â±ï¸ğŸ“‰: Measures training time and evaluates the impact of parallelization on efficiency.
- **Custom CUDA Kernels** ğŸ› ï¸ğŸ”¥: Integrates custom CUDA kernels for data scaling and Mean Squared Error (MSE) computation, enhancing computational performance.

---

## ğŸ“¦ **Install Required Packages**

Open your terminal and install the necessary Python packages:

```bash
pip install boto3
pip install pycuda
pip install pandas
pip install numpy
pip install matplotlib
pip install yfinance
pip install tensorflow
```

---

## ğŸ› ï¸ **Usage**

### ğŸ” **Configure AWS Credentials**

Ensure your AWS credentials are securely set up. Avoid hardcoding them in the notebook. Instead, use environment variables or AWS IAM roles.

```python
import os

os.environ['AWS_ACCESS_KEY_ID'] = 'YOUR_ACCESS_KEY'
os.environ['AWS_SECRET_ACCESS_KEY'] = 'YOUR_SECRET_KEY'
```

### ğŸ““ **Open the Jupyter Notebook**

Launch Jupyter Notebook and open `HPCprojectWorker2_FINAL_with_scaling.ipynb`.

```bash
jupyter notebook
```

### â–¶ï¸ **Run the Notebook**

Execute the cells sequentially. The notebook covers:

- Data downloading and visualization
- GPU-accelerated data preprocessing
- LSTM model definition and training
- Distributed training with multi-worker synchronization
- Performance profiling and results visualization

---

## ğŸ“Š **Results**

### ğŸ•’ **Training Time Comparison**

- **Without Parallelization:**
  - **50 Epochs**: ~7 minutes
  - **100 Epochs**: ~40 minutes

- **With Parallelization:**
  - **50 Epochs**: ~4 minutes
  - **100 Epochs**: ~16 minutes

*Parallelization significantly reduces training time, enhancing efficiency and scalability.*

---

### ğŸ‘¥ **Multi-Worker Strategy**

#### Single Worker vs. Five Workers:

The multi-worker approach accelerates training by distributing the workload across multiple instances. While a single worker takes longer to train, five workers collaboratively reduce the overall training time without compromising the model's predictive accuracy.

---

## ğŸ‘¥ **Multi-Worker Strategy**

Our approach employs a **multi-worker synchronization strategy** to enhance training efficiency and model performance. By leveraging AWS S3 as a centralized storage for model checkpoints, multiple workers can save their model states at defined intervals (e.g., every 5 epochs). These models are then retrieved, and their weights are averaged to create a consensus model. This method ensures consistency across workers, reduces training time, and improves the model's generalization by incorporating diverse training signals.

---

## â±ï¸ **Profiling & Performance**

Performance profiling was conducted to evaluate the impact of parallelization and multi-worker strategies:

- **Training Time Reduction**: Parallelization cuts down the training duration by over 50%, enabling faster experimentation and iteration.
- **Scalability**: The multi-worker setup scales seamlessly with additional resources, maintaining efficiency even as the number of workers increases.
- **Accuracy**: Despite the reduced training time, the model retains high predictive accuracy, demonstrating the effectiveness of distributed training.

---

## ğŸ”’ **Security**

**Important:** Ensure that AWS credentials are managed securely. Avoid hardcoding sensitive information in the codebase. Instead, use environment variables, AWS IAM roles, or AWS Secrets Manager to handle credentials safely.

```python
import os

aws_access_key_id = os.getenv('AWS_ACCESS_KEY_ID')
aws_secret_access_key = os.getenv('AWS_SECRET_ACCESS_KEY')
```

---

We hope this project offers a nice conclusion for our rewarding experience during the High Performance Computing course! ğŸŒŸ
